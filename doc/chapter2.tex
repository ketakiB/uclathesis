\chapter{Preliminaries}

\section{Linear Algebra Tools}

\subsection*{Hankel Matrices}
A Hankel matrix is a matrix $H \in \mathbb{R}^{m\times n}$ with constant skew-diagonals. In other words, the value of the $(i, j)^{\mbox{th}}$ entry of $H$ depends only on the sum $i + j$.
\begin{equation*}
H_{m,n} = \begin{bmatrix}
h_1 & h_2 & \cdots & h_n\\
h_2 & h_3 & \cdots & h_{n+1}\\
\vdots & \vdots & \ddots & \vdots\\
h_m & h_{m+1} & \cdots & h_{m+n-1}
\end{bmatrix}
\end{equation*}
If each entry in the matrix is also a matrix, it is called a block Hankel matrix.


\subsection{Fundamental Matrix Subspaces}
We require two of the fundamental matrix subspaces: the column space and the row space. The column space of a matrix $A \in \mathbb{R}^{m\times n}$ is the set of all linear combinations of the column vectors of $A$. The dimension of the column space is called the rank. The row space of a matrix $A \in \mathbb{R}^{m\times n}$ is the set of all linear combinations of the row vectors of $A$.


\subsection{Projections}


\subsection{Singular Value Decomposition}
Any matrix $A \in \mathbb{R}^{m\times n}$ can be decomposed by a singular value decomposition (SVD) given by
\begin{equation*}
A = U\Sigma V^T
\end{equation*}
where $U \in \mathbb{R}^{m\times m}$ and $V \in \mathbb{R}^{n\times n}$ are orthogonal matrices and $\Sigma \in \mathbb{R}^{m\times n}$ is diagonal matrix of the singular values of $A$ ordered such that
\begin{equation*}
\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_k > 0
\end{equation*}



\section{Linear Systems}

\subsection{Linear Time-Invariant Systems}

\subsection{Deterministic and Stochastic Systems}

\subsection{State Space Representations}
We will consider a combined deterministic-stochastic LTI system written in innovation form as
\begin{subequations}\label{eq:2_innovation}
\begin{equation}x(k+1) = Ax(k) + Bu(k) + Ke(k)\end{equation}
\begin{equation}y(k) = Cx(k) + Du(k) + e(k)\end{equation}
\end{subequations}
where $x_k \in \mathbb{R}^n$ is the system state, $u_k \in \mathbb{R}^m$ is the system input, $y_k \in \mathbb{R}^l$ is the system output, and $e_k \in \mathbb{R}^l$ is the innovation. $A$, $B$, $C$, and $D$ are the system matrices with appropriate dimensions and $K$ is the Kalman filter gain. The system represented in (\ref{eq:2_innovation}) can also be represented in predictor form as
\begin{subequations}\label{eq:2_process}
\begin{equation}x(k+1) = A_Kx(k) + B_Ku(k) + Ky(k)\end{equation}
\begin{equation}y(k) = Cx(k) + Du(k) + e(k)\end{equation}
\end{subequations}
where $A_K = A-KC$ and $B_K = B-KD$.

The systems represented by (\ref{eq:2_innovation}) and (\ref{eq:2_process}) are equivalent from an input/output point of view, but because $A_K$ is guaranteed stable even if the original process matrix $A$ is unstable, the predictor form proves advantageous when considering unstable open-loop systems. We will use the state space model in innovation form to derive the general subspace algorithm for identifying combined deterministic-stochastic LTI systems but will rely on the prediction form of the model when considering identification of closed-loop systems.

\section{Assumptions}

\textbf{[Assumption 1]:} $A_K = A - KC$ is stable (i.e. its eigenvalues lie within the unit circle)

\noindent \textbf{[Assumption 2]:} The system is represented in its minimal form
