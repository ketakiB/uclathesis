\chapter{Subspace Identification Methods}
Subspace identification methods (SIM) provide an approach to identifing LTI systems in their state space form using input/output data. SIMs provide an attractive alternative to Prediction Error Methods (PEM) because of their ability to identify MIMO systems and their non-iterative solution nature, making them suitable for working with large data sets. Traditional SIMs include the canonical variate analysis (CVA) method proposed by Larimore \cite{}, the multi-variable output-error state space (MOESP) method proposed by Verhaegen \cite{}, and the numerical algorithms for subspace state space system identification (N4SID) method proposed by Van Overschee and De Moor \cite{}. A unifying theorem proposed by Van Overschee and De Moor \cite{van1995unifying} links these algorithms through the use of different weighting matrices for each method.  

As described in Van Overschee and De Moor's unifying theorem, all SIMs follow the same general two step procedure. First, estimate the subspace spanned by the columns of the extended observability matrix ($\Gamma$) from input-output data $u_k$, $y_k$. The dimension of $\Gamma$ determines the order $n$ of the estimated system. The system order is determined and $\Gamma$ reduced accordingly. Second, the system matrices ($A$, $B$, $C$, $D$) are determined, either directly from the extended observability matrix or from the system's realized state sequence $X$.

The general subspace identification problem is: given a set of input and output data, estimate the system matrices $A$, $B$, $C$, and $D$ and the Kalman filter gain $K$ to within a similarity transform.

\section{Open-Loop Subspace Identification}
When a system is operating in open-loop (i.e. no feedback), the input data is assumed to be independent of past noise. In this case, the traditional SIMs (MOESP, N4SID, CVA) can be applied without modification. 

\subsection{Extended State Space Model}
A combined deterministic-stochastic LTI system is given in its innovation form
\begin{subequations}\label{eq:3_innovation}
\begin{equation}x(k+1) = Ax(k) + Bu(k) + Ke(k)\end{equation}
\begin{equation}y(k) = Cx(k) + Du(k) + e(k)\end{equation}
\end{subequations}
Based on the state space model in (\ref{eq:3_innovation}), an extended state space model can be formulated as
\begin{equation}\label{eq:3_extended_state_space}
Y_f = \Gamma_f X_k + H_f U_f + G_f E_f
\end{equation}
where the subscript $f$ denotes the future horizon. The extended observability matrix is
\begin{equation}\label{eq:3_extended_observability}
\Gamma_f = \begin{bmatrix}C\\ CA\\ \vdots\\ CA^{f-1}\end{bmatrix}
\end{equation}
and $H_f$ and $G_f$ are Toeplitz matrices of the Markov parameters of the deterministic and stochastic subsystems, respectively
\begin{subequations}\label{eq:3_toeplitz}
\begin{equation}
H_f = \begin{bmatrix}
D & 0 & 0 & \cdots & 0\\
CB & D & 0 & \cdots & 0\\
CAB & CB & D & \cdots & 0\\
\vdots & \vdots  & \vdots & \ddots & \vdots\\
CA^{f-2}B & CA^{f-3}B & CA^{f-4}B & \cdots & D
\end{bmatrix}
\end{equation}
\begin{equation}
G_f = \begin{bmatrix}
I & 0 & 0 & \cdots & 0\\
CK & I & 0 & \cdots & 0\\
CAK & CK & I & \cdots & 0\\
\vdots & \vdots  & \vdots & \ddots & \vdots\\
CA^{f-2}K & CA^{f-3}K & CA^{f-4}K & \cdots & I
\end{bmatrix}
\end{equation}
\end{subequations}

\subsection{Input-Output Data}
We arrange the input data in the following Hankel form:
\begin{equation}\label{eq:3_input}
U_f = \begin{bmatrix}
u(k) & u(k+1) & \cdots & u(k+N-1)\\
u(k+1) & u(k+2) & \cdots & u(k+N)\\
\vdots & \vdots & \ddots & \vdots\\
u(k+f-1) & u(k+f) & \cdots & u(k+f+N-2)
\end{bmatrix}
\end{equation}
We similarly arrange the output data $Y_f$ and noise $E_f$ according to the same Hankel form.

!!!!! Need to discuss past and future splitting !!!!!

\begin{equation*}
Z_p = \begin{bmatrix} U_p\\ Y_p\end{bmatrix}
\end{equation*}

\subsection{Estimation of the Extended Observability Matrix}
Estimate $\Gamma_f$ from (\ref{eq:3_extended_state_space}) via MOESP: linear regression followed by an SVD to estimate EOM.

First, eliminate $U_f$ by post-multiplying (\ref{eq:3_extended_state_space}) by $\Pi_{U_f}^\perp$ (eliminate the influence of the input) giving
\begin{equation}
Y_f\Pi_{U_f}^\perp = \Gamma_f X_k\Pi_{U_f}^\perp + G_f E_f\Pi_{U_f}^\perp
\end{equation}
Recalling $E_f$ is uncorrelated with $U_f$, 
\begin{equation}
Y_f\Pi_{U_f}^\perp = \Gamma_f X_k\Pi_{U_f}^\perp + G_f E_f
\end{equation}
Next we eliminate the influence of the noise term $E_f$. From Kalman filter theory, $E_f$ is uncorrelated with $Z_p$ (cite Qin overview paper on this one):
\begin{equation*}
\lim_{N\rightarrow\infty}\frac{1}{N}E_fZ_p^T =0
\end{equation*}
Thus multiplying (3.7) on the right by $Z_p$ gives
\begin{equation}
Y_f\Pi_{U_f}^\perp Z_p = \Gamma_f X_k\Pi_{U_f}^\perp Z_p
\end{equation}


\subsection{Determination of the System Matrices}




\section{Closed-Loop Subspace Identification}

Under open-loop conditions, $E_f$ is uncorrelated with $U_f$. That is,
\begin{equation*}
\lim_{N\rightarrow\infty}\frac{1}{N}E_fU_f^T =0
\end{equation*}
or 
\begin{equation*}
E_f \Pi_{U_f}^\perp = E_f(I-U_f^T(U_fU_f^T)^{-1}U_f) = E_f
\end{equation*}

\subsection{Identifying Systems Operating Under Feedback Control}

\subsection{Innovation Estimation Method}

\subsection{Whitening Filter}
