\chapter{Subspace Identification Methods}
Subspace identification methods (SIM) provide an approach to identifing LTI systems in their state space form using input-output data. SIMs provide an attractive alternative to Prediction Error Methods (PEM) because of their ability to identify MIMO systems and their non-iterative solution nature, making them suitable for working with large data sets. In general, the subspace identification problem is: given a set of input and output data, estimate the system matrices ($A$, $B$, $C$, $D$) up to within a similarity transform. 

Extensive work in both the theory and application of SIMs in the last 20 years has resulted in the development of a number of popular algorithms, including the canonical variate analysis (CVA) method proposed by Larimore \cite{larimore1990canonical}, the multi-variable output-error state space (MOESP) method proposed by Verhaegen \cite{verhaegen1992subspace}, and the numerical algorithms for subspace state space system identification (N4SID) proposed by Van Overschee and De Moor \cite{van1994n4sid}. A unifying theorem proposed by Van Overschee and De Moor \cite{van1995unifying} links these algorithms and provides a generalized approach to the subspace identification problem.  

As described in Van Overschee and De Moor's unifying theorem, all SIMs follow the same general two step procedure. First, estimate the subspace spanned by the columns of the extended observability matrix ($\Gamma_k$) from input-output data $u_k$, $y_k$. Because the dimension of $\Gamma_k$ determines the order $n$ of the estimated system, we reduce the order of the estimated subspace before proceeding. Second, the system matrices are determined, either directly from the extended observability matrix or from the realized state sequence $X_k$.


\section{Subspace System Identification}
When a system is operating in open-loop (i.e. no feedback), the input data is assumed to be independent of past noise. In this case, the traditional SIMs (MOESP, N4SID, CVA) can be used without modification. 
\begin{figure}[htb!]
	\centering
	\includegraphics{../fig/sim_flow_diagram.pdf}
	\caption{Subspace identification algorithm overview}
\end{figure}


\subsection{Extended State Space Model}\label{sec:extended_state_space_model}
Recalling the combined deterministic-stochastic LTI system is given in its innovation form as
\begin{subequations}\label{eq:3_innovation}
\begin{equation}x(k+1) = Ax(k) + Bu(k) + Ke(k)\end{equation}
\begin{equation}y(k) = Cx(k) + Du(k) + e(k)\end{equation}
\end{subequations}
Application of the subspace algorithms require we represent the input, output, and noise sequences in Hankel form with $2k$ block rows and $N$ columns. For the input matrix,
\begin{equation}\label{eq:3_input}
U_k = \begin{bmatrix}
u(0) & u(1) & \cdots & u(N-1)\\
u(1) & u(2) & \cdots & u(N)\\
\vdots & \vdots & \ddots & \vdots\\
u(k-1) & u(k) & \cdots & u(k+N-2)\\
\hline
u(k) & u(k+1) & \cdots & u(k+N-1)\\
u(k+1) & u(k+2) & \cdots & u(k+N)\\
\vdots & \vdots & \ddots & \vdots\\
u(2k-1) & u(2k) & \cdots & u(2k+N-2)
\end{bmatrix} = 
\begin{bmatrix} U_p\\ \hline U_f\end{bmatrix}
\end{equation}
where $p$ and $f$ denote past and future horizons, respectively. The input matrix is partitioned into these two overlapping ``past'' and ``future'' blocks to later construct an instrumental variable matrix used to eliminate the influence of noise. We delay the explanation of this procedure until it is needed in Section \ref{sec:estimation_of_the_extended_observability_matrix}. We construct similar matrices $Y_k$ and $E_k$ for the output and noise data.

Based on the state space representation in Eq. (\ref{eq:3_innovation}) and considering the future horizon, an extended state space model can be formulated as
\begin{equation}\label{eq:3_extended_state_space}
Y_f = \Gamma_f X_k + H_f U_f + G_f E_f
\end{equation}
The extended observability matrix is
\begin{equation}\label{eq:3_extended_observability}
\Gamma_f = \begin{bmatrix}C\\ CA\\ \vdots\\ CA^{f-1}\end{bmatrix}
\end{equation}
and $H_f$ and $G_f$ are Toeplitz matrices of the Markov parameters of the deterministic and stochastic subsystems, respectively
\begin{subequations}\label{eq:3_toeplitz}
\begin{equation}
H_f = \begin{bmatrix}
D & 0 & 0 & \cdots & 0\\
CB & D & 0 & \cdots & 0\\
CAB & CB & D & \cdots & 0\\
\vdots & \vdots  & \vdots & \ddots & \vdots\\
CA^{f-2}B & CA^{f-3}B & CA^{f-4}B & \cdots & D
\end{bmatrix}
\end{equation}
\begin{equation}
G_f = \begin{bmatrix}
I & 0 & 0 & \cdots & 0\\
CK & I & 0 & \cdots & 0\\
CAK & CK & I & \cdots & 0\\
\vdots & \vdots  & \vdots & \ddots & \vdots\\
CA^{f-2}K & CA^{f-3}K & CA^{f-4}K & \cdots & I
\end{bmatrix}
\end{equation}
\end{subequations}
We will leverage this structure of Eq. (\ref{eq:3_extended_state_space}) to identify the unknown system matrices from known input-output data. In particular, we will estimate the column space of the extended observability matrix. Knowledge of this subspace is sufficient to then recover the unknown system matrices. 


\subsection{Estimation of the Extended Observability Matrix}\label{sec:estimation_of_the_extended_observability_matrix}
Determination of the system matrices relies on an estimate of the column space of $\Gamma_f$. In order to estimate the column space of $\Gamma_f$ in Eq. (\ref{eq:3_extended_state_space}), we must eliminate the influence of the input and noise terms. The general procedure, as outlined in \cite{qin2006overview, verhaegen2007filtering} is as follows: First, we eliminate the influence of the input $U_f$ by post-multiplying Eq. (\ref{eq:3_extended_state_space}) by $\Pi_{U_f}^\perp$ where $\Pi_{U_f}^\perp$ is an orthogonal projection onto the column space of $U_f$ given by
\begin{equation*}
\Pi_{U_f}^\perp = I - U_f^T(U_f U_f^T)^{-1}U_f
\end{equation*}
By definition, $U_f\Pi_{U_f}^\perp = 0$ so Eq. (\ref{eq:3_extended_state_space}) becomes
\begin{equation}
Y_f\Pi_{U_f}^\perp = \Gamma_f X_k\Pi_{U_f}^\perp + G_f E_f\Pi_{U_f}^\perp
\end{equation}
Under open-loop conditions, $E_f$ is uncorrelated with $U_f$. That is,
\begin{equation*}
E_f \Pi_{U_f}^\perp = E_f(I-U_f^T(U_fU_f^T)^{-1}U_f) = E_f
\end{equation*}
so
\begin{equation}\label{eq:3_extended_state_space_noinput}
Y_f\Pi_{U_f}^\perp = \Gamma_f X_k\Pi_{U_f}^\perp + G_f E_f
\end{equation}
Next we eliminate the influence of the noise $E_f$. In order to remove the influence of the noise on the extended observability matrix, we must introduce an instrumental variable matrix as described in \cite{verhaegen2007filtering}. We seek a matrix $Z \in \mathbb{R}^{2k\times N}$ which exhibits the following properties:
\begin{subequations}\begin{equation}\label{eq:3_instrumental_a}
\lim_{N\rightarrow\infty} \frac{1}{N} E_f Z^T = 0
\end{equation}
\begin{equation}\label{eq:3_instrumental_b}
\mbox{rank}\left(\lim_{N\rightarrow\infty} \frac{1}{N} X_k \Pi_{U_f}^\perp Z^T\right) = n
\end{equation}
\end{subequations}
Satisfying condition (\ref{eq:3_instrumental_a}) ensures that we can eliminate $E_f$ by multiplying Eq. (\ref{eq:3_extended_state_space_noinput}) on the right by $Z^T$ and take the limit for $N\rightarrow\infty$:
\begin{equation}\label{eq:3_extended_state_space_noinput_nonoise}
\lim_{N\rightarrow\infty} \frac{1}{N}Y_f\Pi_{U_f}^\perp Z^T = \lim_{N\rightarrow\infty} \frac{1}{N}\Gamma_f X_k\Pi_{U_f}^\perp Z^T
\end{equation}
 Satisfying condition (\ref{eq:3_instrumental_b}) ensures multiplication by $Z^T$ does not change the rank of the remaining term on the right hand side of Eq. (\ref{eq:3_extended_state_space_noinput_nonoise}) so we have
\begin{equation}\label{eq:3_range}
\mbox{range}\left(\lim_{N\rightarrow\infty} \frac{1}{N} Y_f\Pi_{U_f}^\perp Z^T\right) = \mbox{range}\left(\Gamma_f\right)
\end{equation}
From Eq. (\ref{eq:3_range}) we see that an SVD of the matrix $Y_f\Pi_{U_f}^\perp Z^T$  will provide an estimate of the column space of $\Gamma_f$. All that remains is to identify a suitable instrumental variable matrix $Z$. As described in \cite{soderstrom1983instrumental, verhaegen2007filtering}, instrumental variable matrices are typically constructed from input-output data. Recalling that we partitioned our input and output data into ``past'' and ``future'' sets in Section \ref{sec:extended_state_space_model}, we will use the ``future'' input-output data to identify the system and the ``past'' input-output data as the instrumental variable matrix $Z_p$ where
\begin{equation*}
Z_p = \begin{bmatrix}U_p\\ Y_p\end{bmatrix}
\end{equation*}
Recalling that $E_f$ is uncorrelated with $U_f$ for open-loop systems, and enforcing the assumption that $E_f$ is white-noise, we have from \cite{verhaegen2007filtering} that 
\begin{equation*}
\lim_{N\rightarrow\infty} \frac{1}{N} E_f Z_p^T = 0
\end{equation*}
which satisfies condition (\ref{eq:3_instrumental_a}). Jannson showed in \cite{jansson1997subspace} that if the input sequence is persistently exciting, the rank condition (\ref{eq:3_instrumental_b}) is satisfied. We will enforce the persistence of excitation criteria during experiment design to ensure $Z_p$ is a valid instrumental variable matrix.

\subsection{Rank Reduction}
In the presence of noise, the matrix $Y_f\Pi_{U_f}^\perp Z^T$ is full rank while the true system order is smaller. We choose the order of the identified system by partitioning the SVD matrices as follows:
\begin{equation*}
Y_f\Pi_{U_f}^\perp Z^T = 
\begin{bmatrix}U_1 & U_2\end{bmatrix}
\begin{bmatrix}S_1 & 0\\ 0 & S_2\end{bmatrix}
\begin{bmatrix}V_1^T & V_2^T\end{bmatrix}
\end{equation*}
where the number of singular values $n$ in $S_1$ is equal to the system order and the remaining submatrices are scaled appropriately. The reduced rank estimate of the extended observability matrix is then
\begin{equation}
\hat{\Gamma}_f = U_1 {S_1}^{1/2}
\end{equation}


\subsection{Determination of the System Matrices}
In order to recover the system matrices, we follow the general procedure as outlined in \cite{katayama2005subspace}. First, we will exploit the structure of the extended observability matrix to recover the $A$ and $C$ matrices. The matrix $C$ can be read directly from the first block row of $\hat{\Gamma}_f$. In order to recover $A$, we define the following two matrices 
\begin{equation}\label{eq:3_shifted_gamma}
\hat{\overline{\Gamma}}_f = \begin{bmatrix}C\\ \vdots \\ CA^{f-2}\end{bmatrix}, \hspace{3em}
\hat{\underline{\Gamma}}_f = \begin{bmatrix}CA\\ \vdots \\ CA^{f-1}\end{bmatrix}
\end{equation}
where $\hat{\overline{\Gamma}}_f$ is equal to $\hat{\Gamma}_f$ without the last block row and $\hat{\underline{\Gamma}}_f$ is equal to $\hat{\Gamma}_f$ without the first block row. The structure of Eq. (\ref{eq:3_shifted_gamma}) implies
\begin{equation}
\hat{\overline{\Gamma}}_f A = \hat{\underline{\Gamma}}_f
\end{equation}
which is linear in $A$ and can be solved by least squares.

All that remains is to recover the $B$ and $D$ matrices. Recalling the extended state space model is given by
\begin{equation*}
Y_f = \Gamma_f X_k + H_f U_f + G_f E_f
\end{equation*}
and multiplying on the left by $\hat{\Gamma}_f^\perp$ and on the right by ${U_f}^\dagger$ we have
\begin{equation}\label{eq:3_extended_state_space_bd}
\hat{\Gamma}_f^\perp Y_f {U_f}^\dagger = \hat{\Gamma}_f^\perp\Gamma_f X_k {U_f}^\dagger + \hat{\Gamma}_f^\perp H_f U_f {U_f}^\dagger + \hat{\Gamma}_f^\perp G_f E_f {U_f}^\dagger
\end{equation}
where $\hat{\Gamma}_f^\perp$ satisfies $\hat{\Gamma}_f^\perp\hat{\Gamma}_f = 0$ and $\dagger$ denotes the Moore-Penrose pseudoinverse. Equation (\ref{eq:3_extended_state_space_bd}) simplifies to
\begin{equation}\label{eq:3_extended_state_space_bd_simplified}
\hat{\Gamma}_f^\perp Y_f {U_f}^\dagger = \hat{\Gamma}_f^\perp H_f 
\end{equation}
Partitioning $\hat{\Gamma}_f^\perp Y_f {U_f}^\dagger$ into columns with the $i^{\mbox{th}}$ column denoted by $\mathcal{M}_i$ and $\hat{\Gamma}_f^\perp$ into rows with the $i^{\mbox{th}}$ row denoted by $\mathcal{L}_i$, Eq. (\ref{eq:3_extended_state_space_bd_simplified}) is
\begin{equation*}
\begin{bmatrix}\mathcal{M}_1 & \mathcal{M}_2 & \cdots & \mathcal{M}_f\end{bmatrix} = 
\begin{bmatrix}\mathcal{L}_1\\ \mathcal{L}_2\\ \vdots\\ \mathcal{L}_f\end{bmatrix}
\begin{bmatrix}
D & 0 & \cdots & 0\\
CB & D & \cdots & 0\\
\vdots & \vdots  & \ddots & \vdots\\
CA^{f-2}B & CA^{f-3}B & \cdots & D
\end{bmatrix}
\end{equation*}
We can rewrite the above equation as
\begin{equation}
\begin{bmatrix}\mathcal{M}_1\\ \mathcal{M}_2\\ \mathcal{M}_3\\ \vdots\\ \mathcal{M}_f\end{bmatrix} = 
\begin{bmatrix}
\mathcal{L}_1 & \mathcal{L}_2 & \cdots & \mathcal{L}_{f-1} & \mathcal{L}_f\\
\mathcal{L}_2 & \mathcal{L}_3 & \cdots & \mathcal{L}_{f} & 0\\
\mathcal{L}_3 & \mathcal{L}_4 & \cdots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\mathcal{L}_f & 0 & 0 & \cdots & 0
\end{bmatrix}
\begin{bmatrix}I & 0\\ 0 & \hat{\overline{\Gamma}}_f\end{bmatrix}
\begin{bmatrix}D \\ B\end{bmatrix}
\end{equation}
which is an overdetermined linear system in $B$ and $D$. We recover $B$ and $D$ through least squares with $B\in\mathbb{R}$ ???



\subsection{Numerical Efficiencies by LQ Factorization}
In the case where $N$ is large, the construction of the matrix $Y_f\Pi_{U_f}^\perp Z^T$ and the calculation of its SVD are computationally intensive. Verhaegen has shown in \cite{verhaegen1994identification} that from the following LQ factorization
\begin{equation}\label{eq:3_lq}
\begin{bmatrix}U_f\\ U_p\\ Y_p\\ Y_f\end{bmatrix} = 
\begin{bmatrix}
	L_{11} & 0 & 0 & 0\\
	L_{21} & L_{22} & 0 & 0\\
	L_{31} & L_{32} & L_{33} & 0\\
	L_{41} & L_{42} & L_{43} & L_{44}
\end{bmatrix}
\begin{bmatrix}Q_1\\ Q_2\\ Q_3\\ Q_4\end{bmatrix}
\end{equation}
we have
\begin{equation}\label{eq:3_range_lq}
\mbox{range}\left(\lim_{N\rightarrow\infty} \frac{1}{\sqrt{N}} \begin{bmatrix}L_{42} & L_{43}\end{bmatrix}\right) = \mbox{range}\left(\Gamma_f\right)
\end{equation}
This shows an equivalency between Eq. (\ref{eq:3_range}) and Eq. (\ref{eq:3_range_lq}), therefore we can estimate the column space of $\hat{\Gamma}_f$ by computing the LQ factorization in Eq. (\ref{eq:3_lq}), taking the SVD of the matrix $\begin{bmatrix}L_{42} & L_{43}\end{bmatrix}$, and reducing the system order as described above. The remainder of this document assumes $\hat{\Gamma}_f$ is estimated via LQ factorization.









\section{Closed-Loop Subspace Identification}
In many real-world situations, it is either not practical or not possible to collect open-loop input and output data. An unstable system relying on some form of feedback control to operate safely is an example of such a case. It is well known that traditional subspace methods produce biased results in the presence of feedback. This is due to the correlation between the system input and past noise as the controller attempts to eliminate system disturbances \cite{qin2006overview}. As a result, we are not able to fully decouple the input and noise sequences when estimating the subspace of the extended observability matrix when applying traditional SIMs.
\begin{figure}[htb!]
	\centering
	\includegraphics{../fig/closed_loop_block_diagram.pdf}
	\caption{A block diagram of an LTI system operating under feedback control.}
\end{figure}

Recently, several new approaches to identifying closed-loop systems by decoupling inputs from past noise (thus removing any bias) have been proposed. Among these approaches are the Innovation Estimation Method (IEM) proposed by Qin and Ljung \cite{qin2003closed} and the Whitening Filter Approach (WFA) proposed by Chiuso and Picci \cite{chiuso2005consistency}. The IEM pre-estimates the innovation sequence $E_f$ row-wise via a high-order Auto-Regression model with eXogeneous inputs (ARX) algorithm, which is then used to estimate $\Gamma_f$ from Eq. (\ref{eq:3_extended_state_space}). The WFA partitions a modified version of the extended state space model row-wise and estimates $\Gamma_f$ through a multi-stage least squares followed by an SVD. It is worth noting that Chiuso and Picci concluded in \cite{chiuso2005consistency} that while all closed-loop subspace identification algorithms considered produce somewhat biased results in the presence of feed- back control, these algorithms are still able to provide significant improvements over traditional SIMs when identifying closed-loop systems.

\subsection{Estimation of the Extended Observability Matrix by the Whitening Filter Approach}
In order to describe the Whitening Filter Approach, we introduce the predictor form of the system:
\begin{subequations}\label{eq:3_process}
\begin{equation}x(k+1) = A_Kx(k) + B_Ku(k) + Ky(k)\end{equation}
\begin{equation}y(k) = Cx(k) + Du(k) + e(k)\end{equation}
\end{subequations}
where $A_K = A-KC$ and $B_K = B-KD$. Because the input $u(k)$ is determined via feedback, we consider it to be correlated with past innovation $e(k)$. The state $x(k)$ of this form is the same as for the innovation form in Eq. (\ref{eq:2_innovation}), but because $A_K$ is guaranteed stable even if the original process matrix $A$ is unstable, the predictor form proves advantageous when considering open-loop unstable systems \cite{qin2006overview}.

We are able to derive an expression for $X_k$ by iterating Eq. (\ref{eq:3_process})
\begin{equation}\label{eq:3_state}
X_k = L_p Z_p + A_K^p X_{k-p}
\end{equation}
where
\begin{align*}
X_k &= \begin{bmatrix} x(k) & x(k+1) & \cdots & x(k+N-1)\end{bmatrix}\\
L_p &= \begin{bmatrix}L_p^y & L_p^u\end{bmatrix}\\
L_p^u &= \begin{bmatrix}A_K^{p-1}B_K & A_K^{p-2}B_K & \cdots & B_K\end{bmatrix}\\
L_p^y &= \begin{bmatrix}A_K^{p-1}K & A_K^{p-2}K & \cdots & K\end{bmatrix}\\
Z_p &= \begin{bmatrix} Y_p^T & U_p^T\end{bmatrix}^T
\end{align*}

Based on the state space representation in Eq. (\ref{eq:3_process}), we construct the following modified state space model
\begin{equation}\label{eq:3_modified_state_space_model}
Y_f = \overline{\Gamma}_f X_k + \overline{H}_f U_f + \overline{G}_f Y_f + E_f
\end{equation}
with a modified extended observability matrix given by
\begin{equation}\label{eq:3_updated_extended_observability}
\overline{\Gamma}_f = \begin{bmatrix}C\\ CA_K\\ \vdots\\ CA_K^{f-1}\end{bmatrix}
\end{equation}
and Toeplitz matrices $\overline{H}_f$ and $\overline{G}_f$ given by
\begin{subequations}\label{eq:3_updated_toeplitz}
\begin{equation}
\overline{H}_f = \begin{bmatrix}
D & 0 & 0 & \cdots & 0\\
CB_K & D & 0 & \cdots & 0\\
CA_KB_K & CB_K & D & \cdots & 0\\
\vdots & \vdots  & \vdots & \ddots & \vdots\\
CA_K^{f-2}B_K & CA_K^{f-3}B_K & CA_K^{f-4}B_K & \cdots & D
\end{bmatrix}
\end{equation}
\begin{equation}
\overline{G}_f = \begin{bmatrix}
0 & 0 & 0 & \cdots & 0\\
CK & 0 & 0 & \cdots & 0\\
CA_KK & CK & 0 & \cdots & 0\\
\vdots & \vdots  & \vdots & \ddots & \vdots\\
CA_K^{f-2}K & CA_K^{f-3}K & CA_K^{f-4}K & \cdots & 0
\end{bmatrix}
\end{equation}
\end{subequations}

Substituting Eq. (\ref{eq:3_state}) into Eq. (\ref{eq:3_modified_state_space_model}), we have
\begin{equation}\label{eq:3_modified_state_space_model_with_state}
Y_f = \overline{\Gamma}_f L_p Z_p + \overline{\Gamma}_f A_K^p X_{k-p} + \overline{H}_f U_f + \overline{G}_f Y_f + E_f
\end{equation}
If we assume that the eigenvalues of $A_K$ lie strictly within the unit circle, for a sufficiently large $p$, $A_K^p \approx 0$ so Eq. (\ref{eq:3_modified_state_space_model_with_state}) becomes
\begin{equation}\label{eq:3_modified_state_space_model_with_state_no_AK}
Y_f = \overline{\Gamma}_f L_p Z_p + \overline{H}_f U_f + \overline{G}_f Y_f + E_f
\end{equation}
Partitioning Eq. (\ref{eq:3_modified_state_space_model_with_state_no_AK}) row-wise, we have
\begin{equation}\label{eq:3_partitioned_modified_state_space_model}
Y_{fi} = \overline{\Gamma}_{fi} L_p Z_p + \overline{H}_{fi} U_{fi} + \overline{G}_{fi} Y_{fi} + E_{fi}
\end{equation}
where
\begin{align*}
\overline{\Gamma}_{fi} &= CA_K^{i-1}\\
\overline{H}_{fi} &= \begin{bmatrix} CA_K^{i-2}B_K & CA_K^{i-3}B_K & \cdots & CB_K & D\end{bmatrix}\\
\overline{G}_{fi} &= \begin{bmatrix} CA_K^{i-2}K & CA_K^{i-3}K & \cdots & CK & 0\end{bmatrix}
\end{align*}

Using least squares, we estimate $\overline{\Gamma}_{fi}L_p$ for $i = 1, 2, \dots, f$, which then forms $\widehat{\overline{\Gamma}_f L_p}$































